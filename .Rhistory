filter(token %in% table_1$token) %>%
left_join(table_1) %>%
left_join(table_0, by = "token") %>%
group_by(!!id_quo) %>%
summarise(prob_1 = prior_1 + sum(logprob_1), prob_0 = prior_0 + sum(logprob_0), score = mean(score)) %>%
mutate(pred = as.integer(prob_1 > prob_0))
}
get_prob_tables <- function(group_1, group_0, message_col){
message_col <- enquo(message_col)
#word counts for both groups
counts_1 <- get_counts_unique(group_1, !!message_col)
counts_0 <- get_counts_unique(group_0, !!message_col)
allterms <- data.frame(token = unique(c(counts_1$token, counts_0$token)), stringsAsFactors = FALSE)
###Posterior probabilities for each term given the document is spam or ham###
prob_table_1 <- allterms %>% left_join(counts_1, by = "token") %>%
mutate(n = replace(n, is.na(n), 0), n = n + 1)
prob_table_0 <- allterms %>% left_join(counts_0, by = "token") %>%
mutate(n = replace(n, is.na(n), 0), n = n + 1)
prob_table_1 <- prob_table_1 %>% mutate(logprob_1 = log(n/sum(n)))
prob_table_0 <- prob_table_0 %>% mutate(logprob_0 = log(n/sum(n)))
list(prob_table_1, prob_table_0)
}
#get occurences of words within messages for unique email id's
simplify_document <- function(df, string_col, id_col, stop_words = NULL, stop_regex = NULL, stem_terms = NULL, lemmatize = FALSE){
quo_col <- enquo(string_col)
id_col <- enquo(id_col)
stem_pattern = paste(stem_terms, collapse = "|")
df <- df %>%
select(!!id_col, !!quo_col) %>%
unnest_tokens(token, !!quo_col, token = "words") %>%
group_by(!!id_col) %>%
mutate(groupid = 1:n()) %>%
ungroup() %>%
anti_join(stop_words, by = c("token"="word")) %>%
regex_anti_join(stop_regex) %>%
mutate(token = gsub("[[:punct:]]", "", token))
if(!is.null(stem_terms)){
df <- df %>%
mutate(token = ifelse(is.na(str_extract(token, stem_pattern)),
token,
str_extract(token, stem_pattern)))
}
if(lemmatize){
dir = tempdir()
infile = paste0(dir, "\\input.txt")
outfile = paste0(dir, "\\output.txt")
write_delim(df %>% select(token) %>% distinct(), infile)
shell(paste0("cd ", gsub("\\", "/", dir, fixed = TRUE), "&&tag-english input.txt > output.txt"))
output <- read_tsv(outfile) %>% select(token, token_1)
file.remove(outfile)
file.remove(infile)
}
df %>%
left_join(output) %>%
select(!!id_col, groupid, token_1) %>%
group_by(token_1) %>%
filter(n() >= 5) %>%
group_by(!!id_col) %>%
arrange(groupid) %>%
summarise(token = paste(token_1, collapse = " "))
}
get_counts_unique <- function(df, message_col){
message_col <- enquo(message_col)
df %>% select(!!message_col) %>%
unnest_tokens(token, !!message_col, token = "words") %>%
group_by(token) %>%
mutate(n = n()) %>%
unique() %>%
ungroup()
}
get_prob_tables <- function(group_1, group_0, message_col){
message_col <- enquo(message_col)
#word counts for both groups
counts_1 <- get_counts_unique(group_1, !!message_col)
counts_0 <- get_counts_unique(group_0, !!message_col)
allterms <- data.frame(token = unique(c(counts_1$token, counts_0$token)), stringsAsFactors = FALSE)
###Posterior probabilities for each term given the document is spam or ham###
prob_table_1 <- allterms %>% left_join(counts_1, by = "token") %>%
mutate(n = replace(n, is.na(n), 0), n = n + 1)
prob_table_0 <- allterms %>% left_join(counts_0, by = "token") %>%
mutate(n = replace(n, is.na(n), 0), n = n + 1)
prob_table_1 <- prob_table_1 %>% mutate(logprob_1 = log(n/sum(n)))
prob_table_0 <- prob_table_0 %>% mutate(logprob_0 = log(n/sum(n)))
list(prob_table_1, prob_table_0)
}
test_model <- function(test_df, table_1, table_0, message_col, id_col, prior_0 = 0.5, prior_1 = 0.5){
quo_col <- enquo(message_col)
id_quo <- enquo(id_col)
test_df %>%
select(!!id_quo, !!quo_col, score) %>%
unnest_tokens(token, !!quo_col) %>%
filter(token %in% table_1$token) %>%
left_join(table_1) %>%
left_join(table_0, by = "token") %>%
group_by(!!id_quo) %>%
summarise(prob_1 = prior_1 + sum(logprob_1), prob_0 = prior_0 + sum(logprob_0), score = mean(score)) %>%
mutate(pred = as.integer(prob_1 > prob_0))
}
rm(comments_scores_multinomial)
rm(barely)
rm(fraud_messages)
rm(ham_messages)
rm(scores_multinomial)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
source("requirements.R")
comments <- read_delim("Data/toxicity_annotated_comments.tsv",
"\t", escape_double = FALSE, trim_ws = TRUE)
scores <- read_delim("Data/toxicity_annotations.tsv",
"\t", escape_double = FALSE, trim_ws = TRUE)
subs <- "[^[:alnum:][:space:]'{1}]|NEWLINE_TOKEN"
comments <- comments %>% mutate(comment = gsub(subs, " ", comment))
?kable
comments_scores[1:5,] %>% kable("html") %>%
kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))
library(knitr)
comments_scores[1:5,] %>% kable("html") %>%
kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))
library(pander)
comments_scores[1:5,] %>% kable("html") %>%
kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))
??kable_styling
library(kableExtra)
comments_scores[1:5,] %>% kable("html") %>%
kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))
comments_scores[1:5,] %>% kable("html") %>% select(message, split, score) %>%
kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))
comments_scores[1:5,] %>% select(message, split, score) %>% kable("html") %>%
kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))
?kable_styling
comments_good %>% select(message) %>% View()
write_csv(doc_stripped, "Data/corpus_clean.csv")
View(stop_words)
simplify_document <- function(df, string_col, id_col, stop_words = NULL, stop_regex = NULL, stem_terms = NULL, lemmatize = FALSE){
quo_col <- enquo(string_col)
id_col <- enquo(id_col)
#create regex pattern for stemming
stem_pattern = paste(stem_terms, collapse = "|")
#create group id for reassembling step
#remove stop words
#remove regex pattern matches
#
df <- df %>%
select(!!id_col, !!quo_col) %>%
unnest_tokens(token, !!quo_col, token = "words") %>%
group_by(!!id_col) %>%
mutate(groupid = 1:n()) %>%
ungroup() %>%
anti_join(stop_words, by = c("token"="word")) %>%
mutate(token = gsub("[[:punct:]]", "", token))
if(!is.null(stop_regex)){
df <- df %>% regex_anti_join(stop_regex)
}
if(!is.null(stem_terms)){
df <- df %>%
mutate(token = ifelse(is.na(str_extract(token, stem_pattern)),
token,
str_extract(token, stem_pattern)))
}
if(lemmatize){
dir = tempdir()
infile = paste0(dir, "\\input.txt")
outfile = paste0(dir, "\\output.txt")
write_delim(df %>% select(token) %>% distinct(), infile)
shell(paste0("cd ", gsub("\\", "/", dir, fixed = TRUE), "&&tag-english input.txt > output.txt"))
output <- read_tsv(outfile) %>% select(token, token_1)
file.remove(outfile)
file.remove(infile)
}
df %>%
left_join(output) %>%
select(!!id_col, groupid, token_1) %>%
group_by(token_1) %>%
filter(n() >= 5) %>%
group_by(!!id_col) %>%
arrange(groupid) %>%
summarise(token = paste(token_1, collapse = " "))
}
probs[[1]]
write_csv(probs[[1]], "Data/toxic_probs.csv")
write_csv(probs[[2]], "Data/good_probs.csv")
doc_stripped[1:5,] %>% kable("html") %>%
kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))
?wordcloud2
wordcloud2(probs[[1]] %>% top_n(50, wt = logprob_1) %>% select(token, n),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(probs[[2]] %>% top_n(50, wt = logprob_1) %>% select(token, n),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(probs[[2]] %>% top_n(50, wt = logprob_0) %>% select(token, n),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(probs[[1]] %>% top_n(50, wt = logprob_1) %>% select(token, n),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(probs[[2]] %>% top_n(50, wt = logprob_0) %>% select(token, n),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
fulltable <- inner_join(probs[[1]], probs[[2]], by = "token") %>% mutate(ratio = logprob_1/logprob_0)
wordcloud2(fulltable %>% top_n(75, wt = 1/ratio) %>% select(token, n.y),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>% filter(n.x > 1, n.y > 1) %>% top_n(75, wt = ratio) %>% select(token, n.y),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>% filter(n.x > 1, n.y > 1) %>% top_n(75, wt = ratio) %>% select(token, n.y),
size = 0.2,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>% filter(n.x > 1, n.y > 1) %>% top_n(75, wt = ratio) %>% select(token, n.y),
size = 1,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
?order
?rank
View(fulltable)
doc_stripped_scores <- doc_stripped %>% left_join(comments_scores, by = "rev_id") %>% select(rev_id, token, score)
View(doc_stripped_scores)
allprobs <- get_prob_tables(doc_stripped_scores %>% filter(score == -1), doc_stripped_scores %>% filter(score == 1), message)
get_prob_tables <- function(group_1, group_0, message_col){
message_col <- enquo(message_col)
#word counts for both groups
counts_1 <- get_counts_unique(group_1, !!message_col)
counts_0 <- get_counts_unique(group_0, !!message_col)
allterms <- data.frame(token = unique(c(counts_1$token, counts_0$token)), stringsAsFactors = FALSE)
###Posterior probabilities for each term given the document is spam or ham###
prob_table_1 <- allterms %>% left_join(counts_1, by = "token") %>%
mutate(n = replace(n, is.na(n), 0), n = n + 1)
prob_table_0 <- allterms %>% left_join(counts_0, by = "token") %>%
mutate(n = replace(n, is.na(n), 0), n = n + 1)
prob_table_1 <- prob_table_1 %>% mutate(logprob_1 = log(n/sum(n)))
prob_table_0 <- prob_table_0 %>% mutate(logprob_0 = log(n/sum(n)))
list(prob_table_1, prob_table_0)
}
allprobs <- get_prob_tables(doc_stripped_scores %>% filter(score == -1), doc_stripped_scores %>% filter(score == 1), token)
fulltable <- inner_join(allprobs[[1]], allprobs[[2]], by = "token") %>% mutate(ratio = logprob_1/logprob_0)
wordcloud2(fulltable %>% filter(n.x > 1, n.y > 1) %>% top_n(75, wt = ratio) %>% select(token, n.y),
size = 1,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = ratio) %>%
mutate(rank = rank(n.y)) %>%
select(token, rank),
size = 1,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = ratio) %>%
mutate(rank = rank(n.y)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
?wordcloud2
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = 1/ratio) %>%
mutate(rank = rank(n.x)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
doc_strippedd <- read.csv("Data/corpus_clean.csv")
doc_strippedd <- read_csv("Data/corpus_clean.csv")
rm(doc_strippedd)
doc_stripped_scores <- doc_stripped %>% left_join(comments_scores, by = "rev_id") %>% select(rev_id, token, score)
comments <- read_delim("Data/toxicity_annotated_comments.tsv",
"\t", escape_double = FALSE, trim_ws = TRUE)
scores <- read_delim("Data/toxicity_annotations.tsv",
"\t", escape_double = FALSE, trim_ws = TRUE)
doc_stripped <- read_csv("Data/corpus_clean.csv")
subs <- "[^[:alnum:][:space:]'{1}]|NEWLINE_TOKEN"
comments <- comments %>% mutate(comment = gsub(subs, " ", comment))
scores_collapsed <- scores %>% group_by(rev_id) %>%
mutate(score = ifelse(sum(toxicity_score) < -1, 1, 0)) %>%
slice(1)
comments_scores <- comments %>% left_join(scores_collapsed) %>%
select(-toxicity_score, -toxicity)
comments_scores[1:5,] %>% select(message, split, score) %>% kable("html") %>%
kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))
comments_scores[1:5,] %>% select(comment, split, score) %>% kable("html") %>%
kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = ratio) %>%
mutate(rank = rank(n.y)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = 1/ratio) %>%
mutate(rank = rank(n.x)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = 1/ratio) %>%
mutate(rank = rank(n.x), token = "*****") %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
?sample
sample(c("!","*", "you"), 1)
sample(c("!","*", "you"), 1)
sample(c("!","*", "you"), 1)
sample(c("!","*", "you"), 1)
sample(c("!","*", "you"), 1)
sample(c("!","*", "you"), 1)
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = 1/ratio) %>%
mutate(rank = rank(n.x), token = gsub("[A-Za-z]", sample(c("!","*"), 1), token)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = 1/ratio) %>%
mutate(rank = rank(n.x), token = gsub("[A-Za-z]", "*", token)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = 1/ratio) %>%
mutate(rank = rank(n.x), token = gsub("[A-Za-z0-9]", "*", token)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = 1/ratio) %>%
mutate(rank = rank(n.x), token = gsub("[A-Za-z0-9]", "*!", token)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = ratio) %>%
mutate(rank = rank(n.y)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = ratio) %>%
mutate(rank = rank(n.y)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = 1/ratio) %>%
mutate(rank = rank(n.x), token = gsub("[A-Za-z0-9]", "*!", token)) %>% #remove token = ... to uncensor
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
?replace
doc_stripped_scores <- doc_stripped %>%
left_join(comments_scores, by = "rev_id") %>%
select(rev_id, token, score) %>%
mutate(score = ifelse(score == -1, 1,0))
View(doc_stripped_scores)
?ifelse
doc_stripped_scores <- doc_stripped %>%
left_join(comments_scores, by = "rev_id") %>%
select(rev_id, token, score) %>%
mutate(score = ifelse(score <0, 1,0))
View(doc_stripped_scores)
doc_stripped_scores <- doc_stripped %>%
left_join(comments_scores, by = "rev_id") %>%
select(rev_id, token, score) %>%
mutate(score = ifelse(score <0, 1,-1))
View(doc_stripped_scores)
doc_stripped_scores <- doc_stripped %>%
left_join(comments_scores, by = "rev_id") %>%
select(rev_id, token, score) %>%
byrow() %>%
mutate(score = ifelse(score <0, 1,0))
doc_stripped_scores <- doc_stripped %>%
left_join(comments_scores, by = "rev_id") %>%
select(rev_id, token, score) %>%
by_row() %>%
mutate(score = ifelse(score <0, 1,0))
doc_stripped_scores <- doc_stripped %>%
left_join(comments_scores, by = "rev_id") %>%
select(rev_id, token, score) %>%
rowwise() %>%
mutate(score = ifelse(score <0, 1,0))
View(doc_stripped_scores)
?apply
doc_stripped_scores <- doc_stripped %>%
left_join(comments_scores, by = "rev_id") %>%
select(rev_id, token, score)
View(doc_stripped_scores)
View(comments_scores)
good_train <- doc_stripped_scores %>%
filter(score == 1) %>%
sample_frac(0.8)
?sample_frac
good_train <- doc_stripped_scores %>%
filter(score == 0) %>%
sample_frac(0.8)
View(doc_stripped_scores)
good_test <- good_train %>% anti_join()
good_test <- doc_stripped_scores %>% anti_join(good_train)
toxic_test <- doc_stripped_scores %>% anti_join(toxic_train)
allprobs <- get_prob_tables(toxic_train, good_train, token)
View(good_train)
toxic_train <- doc_stripped_scores %>%
filter(score == 1) %>%
sample_frac(0.8)
toxic_test <- doc_stripped_scores %>% anti_join(toxic_train)
allprobs <- get_prob_tables(toxic_train, good_train, token)
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = ratio) %>%
mutate(rank = rank(n.y)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = 1/ratio) %>%
mutate(rank = rank(n.x)) %>% #remove token = ... to uncensor
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
fulltable <- inner_join(allprobs[[1]], allprobs[[2]], by = "token") %>% mutate(ratio = logprob_1/logprob_0)
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = ratio) %>%
mutate(rank = rank(n.y)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = 1/ratio) %>%
mutate(rank = rank(n.x)) %>% #remove token = ... to uncensor
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
preds <- test_model(rbind(toxic_test, good_test), allprobs[[1]], allprobs[[2]], prior_0 = p_good, prior_1 = p_toxic)
test_model <- function(test_df, table_1, table_0, message_col, id_col, prior_0 = 0.5, prior_1 = 0.5){
quo_col <- enquo(message_col)
id_quo <- enquo(id_col)
test_df %>%
select(!!id_quo, !!quo_col, score) %>%
unnest_tokens(token, !!quo_col) %>%
filter(token %in% table_1$token) %>%
left_join(table_1) %>%
left_join(table_0, by = "token") %>%
group_by(!!id_quo) %>%
summarise(prob_1 = prior_1 + sum(logprob_1), prob_0 = prior_0 + sum(logprob_0), score = mean(score)) %>%
mutate(pred = as.integer(prob_1 > prob_0))
}
preds <- test_model(rbind(toxic_test, good_test), allprobs[[1]], allprobs[[2]], token, rev_id, prior_0 = p_good, prior_1 = p_toxic)
View(preds)
table(preds$score, preds$pred)
toxic_test <- doc_stripped_scores %>% anti_join(toxic_train)
good_train <- doc_stripped_scores %>%
filter(score == 0) %>%
sample_frac(0.8)
good_test <- doc_stripped_scores %>%
filter(score == 0) %>%
anti_join(good_train)
toxic_train <- doc_stripped_scores %>%
filter(score == 1) %>%
sample_frac(0.8)
toxic_test <- doc_stripped_scores %>%
filter(score == 1) %>%
anti_join(toxic_train)
allprobs <- get_prob_tables(toxic_train, good_train, token)
fulltable <- inner_join(allprobs[[1]], allprobs[[2]], by = "token") %>% mutate(ratio = logprob_1/logprob_0)
wordcloud2(fulltable %>%
filter(n.x > 1, n.y > 1) %>%
top_n(75, wt = ratio) %>%
mutate(rank = rank(n.y)) %>%
select(token, rank),
size = 0.5,
color  = "#2D8DD6",
backgroundColor = "#E9EDEF")
preds <- test_model(rbind(toxic_test, good_test), allprobs[[1]], allprobs[[2]], token, rev_id, prior_0 = p_good, prior_1 = p_toxic)
table(preds$score, preds$pred)
(24548+3684)/(26834+4858)
