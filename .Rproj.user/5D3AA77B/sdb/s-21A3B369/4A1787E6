{
    "collab_server" : "",
    "contents" : "\n#get occurences of words within messages for unique email id's\nsimplify_document <- function(df, join_col, stop_words = NULL, stop_regex = NULL, stem_terms = NULL){\n  df <- df %>% rowid_to_column()\n  quo_col <- enquo(join_col)\n  stem_pattern = paste(stem_terms, collapse = \"|\")\n\n  df <- df %>%\n    select(rowid, !!quo_col) %>%\n    unnest_tokens(token, message, token = \"words\") %>%\n    group_by(rowid) %>%\n    mutate(groupid = 1:n()) %>%\n    ungroup() %>%\n    anti_join(stop_words, by = c(\"token\"=\"word\")) %>%\n    regex_anti_join(stop_regex) %>%\n    mutate(token = gsub(\"[[:punct:]]\", \"\", token))\n\n  if(!is.null(stem_terms)){\n\n    df <- df %>%\n      mutate(token = ifelse(is.na(str_extract(token, stem_pattern)),\n                            token,\n                            str_extract(token, stem_pattern)))\n  }\n\n\n\n  dir = tempdir()\n  infile = paste0(dir, \"\\\\input.txt\")\n  outfile = paste0(dir, \"\\\\output.txt\")\n  write_delim(df %>% select(token) %>% distinct(), infile)\n  shell(paste0(\"cd \", gsub(\"\\\\\", \"/\", dir, fixed = TRUE), \"&&tag-english input.txt > output.txt\"))\n  output <- read_tsv(outfile) %>% select(token, token_1)\n\n  file.remove(outfile)\n\n  df <- df %>%\n    left_join(output) %>%\n    select(rowid, groupid, token_1) %>%\n    rename(token = token_1) %>%\n    group_by(rowid) %>%\n    arrange(groupid) %>%\n    summarise(token = paste(token, collapse = \" \"))\n\n  df\n\n}\n\nget_counts_unique <- function(df, join_col, stop_words = NULL, stop_regex = NULL, stem_terms = NULL, lemmatize = FALSE){\n\n  df <- df %>% rownames_to_column()\n  quo_col <- enquo(join_col)\n  stem_pattern = paste(stem_terms, collapse = \"|\")\n\n\n    frame <- df %>%\n      select(rowname, !!quo_col) %>%\n      unnest_tokens(token, message, token = \"words\") %>%\n      anti_join(stop_words, by = c(\"token\"=\"word\")) %>%\n      regex_anti_join(stop_regex) %>%\n      mutate(token = gsub(\"[[:punct:]]\", \"\", token))\n\n    if(!is.null(stem_terms)){\n\n      frame <- frame %>%\n               mutate(token = ifelse(is.na(str_extract(token, stem_pattern)),\n                                               token,\n                                               str_extract(token, stem_pattern)))\n    }\n\n\n\n    frame <- frame %>%\n      group_by(token) %>%\n      mutate(n = n()) %>%\n      filter(n > 1) %>%\n      distinct(token, n) %>%\n      ungroup()\n\n    frame\n}\n\nget_prob_tables <- function(spam_train, ham_train){\n  #word counts for both groups\n  word_counts_spam <- get_counts_unique(spam_train, message, ngrams)\n  word_counts_ham <- get_counts_unique(ham_train, message, multinomial, ngrams)\n\n  allterms <- data.frame(token = unique(c(word_counts_spam$token, word_counts_ham$token)), stringsAsFactors = FALSE)\n\n  ###Posterior probabilities for each term given the document is spam or ham###\n    prob_table_spam <- allterms %>% left_join(word_counts_spam, by = \"token\") %>%\n      mutate(n = replace(n, is.na(n), 0), n = n + 1)\n\n    prob_table_ham <- allterms %>% left_join(word_counts_ham, by = \"token\") %>%\n      mutate(n = replace(n, is.na(n), 0), n = n + 1)\n\n    prob_table_spam <- prob_table_spam %>% mutate(logprob = log(n/sum(n)))\n    prob_table_ham <- prob_table_ham %>% mutate(logprob = log(n/sum(n)))\n\n    list(prob_table_spam, prob_table_ham)\n\n}\n\n#test the model given a spam and ham table.\ntest_model <- function(test_emails, spam_table, ham_table, ngrams = FALSE, multinomial = FALSE, prior_ham = 0.5, prior_spam = 0.5){\n\n  test_emails[\"pred\"] <- 0\n\n  for(i in 1:nrow(test_emails)){\n\n        test_mail <- unnest_tokens(test_emails[i,], token, message) %>%\n            select(token) %>%\n            anti_join(stop_words, by = c(\"token\"=\"word\")) %>%\n            filter(token %in% spam_table$token) %>%\n            ungroup()\n\n    ###should be same for both\n    log_prob_ham <- log(prior_ham) +\n      sum((test_mail %>%\n             left_join(ham_table, by = \"token\"))$logprob)\n\n    log_prob_spam <- log(prior_spam) +\n      sum((test_mail %>%\n             left_join(spam_table, by = \"token\"))$logprob)\n\n\n    if(log_prob_spam > log_prob_ham){\n      test_emails[i,]$pred <- 1\n    }\n\n  }\n\n  test_emails\n\n}\n\n\n\n\n",
    "created" : 1517882158905.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "907778393",
    "id" : "4A1787E6",
    "lastKnownWriteTime" : 1517902122,
    "last_content_update" : 1517902122507,
    "path" : "~/Git Repos/bayesclassifier/Configuration/functions.R",
    "project_path" : "Configuration/functions.R",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}